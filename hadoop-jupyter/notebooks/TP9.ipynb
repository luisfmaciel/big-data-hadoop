{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 513,
   "id": "d961fdce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import datetime as dt\n",
    "from multiprocessing import Process\n",
    "\n",
    "import findspark\n",
    "findspark.init()\n",
    "from pathlib import Path\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, LongType, TimestampType\n",
    "from pyspark.sql.functions import col, regexp_replace, row_number, when, sum, count, round, date_format, lit, first\n",
    "from pyspark.sql import SparkSession, Window\n",
    "findspark.init('/opt/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "id": "651bbd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "curl is already the newest version (7.58.0-2ubuntu3.24).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 0 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "! apt-get install -y curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 515,
   "id": "7eb93138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1056k  100 1056k    0     0   599k      0  0:00:01  0:00:01 --:--:--  599k\n"
     ]
    }
   ],
   "source": [
    "! curl -O https://jdbc.postgresql.org/download/postgresql-42.6.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 516,
   "id": "5215ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\n",
    "    \"tp9_test1\"\n",
    ").config(\n",
    "    \"spark.jars\",\n",
    "    f\"{str(Path.cwd())}/postgresql-42.6.0.jar\"\n",
    ").config(\n",
    "    \"spark.driver.extraClassPath\",\n",
    "    f\"{str(Path.cwd())}/postgresql-42.6.0.jar\"\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 517,
   "id": "99205154",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir -p /user/root/bus/batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 518,
   "id": "db068786",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir -p /user/root/bus/stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 519,
   "id": "a896754f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 2 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2023-09-16 21:35 /user/root/bus/stream/20230916_183539\r\n",
      "drwxr-xr-x   - root supergroup          0 2023-09-16 21:36 /user/root/bus/stream/20230916_183642\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /user/root/bus/stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "id": "fa4c3e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/bus/stream/20230916_183828\r\n",
      "Deleted /user/root/bus/stream/20230916_183931\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -skipTrash -r /user/root/bus/stream/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "id": "ee402bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(df):\n",
    "    return (df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "id": "5f6107a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://dados.mobilidade.rio/gps/sppo?\" \\\n",
    "           \"dataInicial={dt_inicial}+{hora_inicial}&dataFinal={dt_final}+{hora_final}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "id": "f6d9ed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bus_api(interval=60):\n",
    "    offset = dt.timedelta(hours=3)\n",
    "    dtf = dt.datetime.now() - offset\n",
    "    dti = dtf - dt.timedelta(seconds=interval)\n",
    "    dt_inicial = dti.strftime(\"%Y-%m-%d\")\n",
    "    dt_final = dtf.strftime(\"%Y-%m-%d\")\n",
    "    hora_inicial = dti.strftime(\"%H:%M:%S\")\n",
    "    hora_final = dtf.strftime(\"%H:%M:%S\")\n",
    "    ret = requests.get(\n",
    "        BASE_URL.format(\n",
    "            dt_inicial=dt_inicial, dt_final=dt_final, hora_inicial=hora_inicial, hora_final=hora_final\n",
    "        )\n",
    "    )\n",
    "    rdd = spark.sparkContext.parallelize([ret.text])\n",
    "    df = spark.read.json(rdd)\n",
    "\n",
    "    df.write.json(f\"/user/root/bus/stream/{dtf.strftime('%Y%m%d_%H%M%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "id": "e007be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\n",
    "    \"traffic_vs_garage\"\n",
    "#     \"percent_above_60km\",\n",
    "#     \"qty_bus_running_by_company\",\n",
    "#     \"qty_bus_running_by_line\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "id": "ef88cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(df):\n",
    "    for table in tables:\n",
    "        jdbc_url = \"jdbc:postgresql://pg-data:5433/postgres\"\n",
    "        database_table = f\"\\\"{table}\\\"\" \n",
    "        properties = {\n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"postgres\",\n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "\n",
    "        df.write.jdbc(url=jdbc_url, table=database_table, mode=\"append\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "id": "86d68a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_traffic(df, dtt):\n",
    "    window = Window.orderBy(\"linha\")\n",
    "    df = df.withColumn(\n",
    "        \"idx\", row_number().over(window)\n",
    "    ).withColumn(\n",
    "        \"garage_line\", when(col(\"linha\") == \"GARAGEM\", 1).otherwise(0)\n",
    "    )\n",
    "    \n",
    "    df = df.agg(\n",
    "        round((sum(\"garage_line\") / count(\"*\") * 100), 2).alias(\"percent_garage\"),\n",
    "        round(((count(\"*\") - sum(\"garage_line\")) / count(\"*\") * 100), 2).alias(\"percent_traffic\"),\n",
    "    ).withColumn(\n",
    "        \"hora\", lit(dtt).cast(StringType())\n",
    "    )\n",
    "\n",
    "    df = df.select(\n",
    "        \"hora\", \"percent_garage\", \"percent_traffic\"\n",
    "    )\n",
    "        \n",
    "    df.show()\n",
    "    \n",
    "    create_table(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "id": "c50e5bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_stream(bdf, ctrl):\n",
    "    offset = dt.timedelta(hours=3)\n",
    "    dtt = dt.datetime.now() - offset\n",
    "\n",
    "    dtt = dtt.strftime('%H:%M:%S')\n",
    "    calc_traffic(bdf, dtt)\n",
    "       \n",
    "#     dest_folder = \"/user/root/bus/test.parquet\"\n",
    "#     df.write.parquet(dest_folder)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "id": "ac25e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema = StructType([\n",
    "#     StructField(\"ordem\", StringType(), True),\n",
    "#     StructField(\"latitude\", StringType(), True),\n",
    "#     StructField(\"longitude\", StringType(), True),\n",
    "#     StructField(\"datahora\", StringType(), True),\n",
    "#     StructField(\"velocidade\", StringType(), True),\n",
    "#     StructField(\"linha\", StringType(), True),\n",
    "#     StructField(\"datahoraenvio\", StringType(), True),\n",
    "#     StructField(\"datahoraservidor\", StringType(), True),\n",
    "# ])\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"idx\", StringType(), True),\n",
    "    StructField(\"linha\", StringType(), True),\n",
    "    StructField(\"percent_garage\", StringType(), True),\n",
    "    StructField(\"percent_traffic\", StringType(), True),\n",
    "    StructField(\"hora\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "id": "56ea6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(max_iter=2):\n",
    "    idx = 1\n",
    "    while True:\n",
    "        print(f\"{idx} - fetching API\")\n",
    "        call_bus_api()\n",
    "        if idx == max_iter:\n",
    "            break\n",
    "        time.sleep(60)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "id": "2bfd8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.readStream.schema(schema).json(\"/user/root/bus/stream/*/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "id": "b810c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_handle = sdf.writeStream                         \\\n",
    "                   .outputMode(\"append\")                \\\n",
    "                   .foreachBatch(handle_stream)         \\\n",
    "                   .trigger(processingTime=\"1 minute\") \\\n",
    "                   .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 549,
   "id": "a56420c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 549,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_handle.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "3ee52f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Terminated with exception: An exception was raised by the Python Proxy. Return Message: Traceback (most recent call last):\\n  File \"/opt/spark/python/pyspark/sql/utils.py\", line 63, in deco\\n    return f(*a, **kw)\\n  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\", line 328, in get_return_value\\n    format(target_id, \".\", name), value)\\npy4j.protocol.Py4JJavaError: An error occurred while calling o5297.jdbc.\\n: org.apache.spark.sql.AnalysisException: Column \"hora\" not found in schema Some(StructType(StructField(percent_garage,DoubleType,true), StructField(percent_traffic,DoubleType,true)));\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$4$$anonfun$6.apply(JdbcUtils.scala:147)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$4$$anonfun$6.apply(JdbcUtils.scala:147)\\n\\tat scala.Option.getOrElse(Option.scala:121)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$4.apply(JdbcUtils.scala:146)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$$anonfun$4.apply(JdbcUtils.scala:145)\\n\\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\tat scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:234)\\n\\tat scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)\\n\\tat scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:186)\\n\\tat scala.collection.TraversableLike$class.map(TraversableLike.scala:234)\\n\\tat scala.collection.mutable.ArrayOps$ofRef.map(ArrayOps.scala:186)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.getInsertStatement(JdbcUtils.scala:145)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcUtils$.saveTable(JdbcUtils.scala:830)\\n\\tat org.apache.spark.sql.execution.datasources.jdbc.JdbcRelationProvider.createRelation(JdbcRelationProvider.scala:68)\\n\\tat org.apache.spark.sql.execution.datasources.SaveIntoDataSourceCommand.run(SaveIntoDataSourceCommand.scala:45)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult$lzycompute(commands.scala:70)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.sideEffectResult(commands.scala:68)\\n\\tat org.apache.spark.sql.execution.command.ExecutedCommandExec.doExecute(commands.scala:86)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:136)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$execute$1.apply(SparkPlan.scala:132)\\n\\tat org.apache.spark.sql.execution.SparkPlan$$anonfun$executeQuery$1.apply(SparkPlan.scala:160)\\n\\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\\n\\tat org.apache.spark.sql.execution.SparkPlan.executeQuery(SparkPlan.scala:157)\\n\\tat org.apache.spark.sql.execution.SparkPlan.execute(SparkPlan.scala:132)\\n\\tat org.apache.spark.sql.execution.QueryExecution.toRdd$lzycompute(QueryExecution.scala:83)\\n\\tat org.apache.spark.sql.execution.QueryExecution.toRdd(QueryExecution.scala:81)\\n\\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\\n\\tat org.apache.spark.sql.DataFrameWriter$$anonfun$runCommand$1.apply(DataFrameWriter.scala:696)\\n\\tat org.apache.spark.sql.execution.SQLExecution$$anonfun$withNewExecutionId$1.apply(SQLExecution.scala:80)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withSQLConfPropagated(SQLExecution.scala:127)\\n\\tat org.apache.spark.sql.execution.SQLExecution$.withNewExecutionId(SQLExecution.scala:75)\\n\\tat org.apache.spark.sql.DataFrameWriter.runCommand(DataFrameWriter.scala:696)\\n\\tat org.apache.spark.sql.DataFrameWriter.saveToV1Source(DataFrameWriter.scala:305)\\n\\tat org.apache.spark.sql.DataFrameWriter.save(DataFrameWriter.scala:291)\\n\\tat org.apache.spark.sql.DataFrameWriter.jdbc(DataFrameWriter.scala:535)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\\n\\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\\n\\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\\n\\tat java.lang.reflect.Method.invoke(Method.java:498)\\n\\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\\n\\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\\n\\tat py4j.Gateway.invoke(Gateway.java:282)\\n\\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\\n\\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\\n\\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\\n\\tat java.lang.Thread.run(Thread.java:750)\\n\\n\\nDuring handling of the above exception, another exception occurred:\\n\\nTraceback (most recent call last):\\n  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 2381, in _call_proxy\\n    return_value = getattr(self.pool[obj_id], method)(*params)\\n  File \"/opt/spark/python/pyspark/sql/utils.py\", line 191, in call\\n    raise e\\n  File \"/opt/spark/python/pyspark/sql/utils.py\", line 188, in call\\n    self.func(DataFrame(jdf, self.sql_ctx), batch_id)\\n  File \"<ipython-input-544-c7b4efd7e9a3>\", line 6, in handle_stream\\n    calc_traffic(bdf, dtt)\\n  File \"<ipython-input-543-30047005124d>\", line 22, in calc_traffic\\n    create_table(df)\\n  File \"<ipython-input-542-23431673b08f>\", line 11, in create_table\\n    df.write.jdbc(url=jdbc_url, table=database_table, mode=\"append\", properties=properties)\\n  File \"/opt/spark/python/pyspark/sql/readwriter.py\", line 990, in jdbc\\n    self.mode(mode)._jwrite.jdbc(url, table, jprop)\\n  File \"/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\\n    answer, self.gateway_client, self.target_id, self.name)\\n  File \"/opt/spark/python/pyspark/sql/utils.py\", line 69, in deco\\n    raise AnalysisException(s.split(\\': \\', 1)[1], stackTrace)\\npyspark.sql.utils.AnalysisException: \\'Column \"hora\" not found in schema Some(StructType(StructField(percent_garage,DoubleType,true), StructField(percent_traffic,DoubleType,true)));\\'\\n',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 552,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_handle.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "id": "711feca6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '44d1e108-7f64-4e6b-a1d7-4ca614028968',\n",
       " 'runId': '91ed356e-fad5-4703-82f2-ae0188d8414e',\n",
       " 'name': None,\n",
       " 'timestamp': '2023-09-16T21:38:23.761Z',\n",
       " 'batchId': 0,\n",
       " 'numInputRows': 0,\n",
       " 'processedRowsPerSecond': 0.0,\n",
       " 'durationMs': {'getOffset': 1, 'triggerExecution': 3},\n",
       " 'stateOperators': [],\n",
       " 'sources': [{'description': 'FileStreamSource[hdfs://node-master:9000/user/root/bus/stream/*/*]',\n",
       "   'startOffset': None,\n",
       "   'endOffset': None,\n",
       "   'numInputRows': 0,\n",
       "   'processedRowsPerSecond': 0.0}],\n",
       " 'sink': {'description': 'ForeachBatchSink'}}"
      ]
     },
     "execution_count": 534,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_handle.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "id": "a953dc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_handle.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "id": "dc737885",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - fetching API\n",
      "+--------+--------------+---------------+\n",
      "|    hora|percent_garage|percent_traffic|\n",
      "+--------+--------------+---------------+\n",
      "|18:41:00|           4.4|           95.6|\n",
      "+--------+--------------+---------------+\n",
      "\n",
      "2 - fetching API\n"
     ]
    }
   ],
   "source": [
    "p = Process(target=loop)\n",
    "p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "34e5dd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+-------------+----------+-----+-------------+----------------+\n",
      "| ordem| latitude|longitude|     datahora|velocidade|linha|datahoraenvio|datahoraservidor|\n",
      "+------+---------+---------+-------------+----------+-----+-------------+----------------+\n",
      "|D13078|-22.87882|-43.46885|1694718086000|         0|  794|1694718101000|   1694718109000|\n",
      "|C30341|-22.99545|-43.36563|1694718090000|        13|  565|1694718101000|   1694718109000|\n",
      "|C47464|-22.98373|-43.21799|1694718084000|         0|  553|1694718101000|   1694718109000|\n",
      "|D13284|-23.01256|-43.31864|1694718060000|        10|  878|1694718101000|   1694718109000|\n",
      "|C47755|-22.92302|-43.23248|1694718090000|         9|  600|1694718101000|   1694718109000|\n",
      "|D13310|-22.89362|-43.45436|1694718082000|        17|  794|1694718101000|   1694718109000|\n",
      "|C47672|-22.97637|-43.41434|1694718083000|         0|  348|1694718101000|   1694718109000|\n",
      "|C47603|-22.91503|-43.38401|1694718088000|        27|  600|1694718101000|   1694718109000|\n",
      "|C30365|-23.00029|-43.33741|1694718084000|        55|  343|1694718101000|   1694718109000|\n",
      "|C47540|-22.97199|-43.49404|1694717999000|        29|  613|1694718101000|   1694718109000|\n",
      "|D13335|-22.87556|-43.41923|1694718086000|         0|  746|1694718101000|   1694718109000|\n",
      "|D13138|-22.87792|-43.41826|1694718087000|        46|  746|1694718101000|   1694718109000|\n",
      "|C30237| -22.9792|-43.33344|1694718086000|         0|  557|1694718101000|   1694718109000|\n",
      "|C47720|-22.91586| -43.4207|1694717893000|         8|  601|1694718101000|   1694718109000|\n",
      "|C47754|-22.91961|-43.37274|1694718087000|         0|  600|1694718101000|   1694718109000|\n",
      "|C47760|-22.93916|-43.34156|1694718013000|         0|  600|1694718101000|   1694718109000|\n",
      "|C47916|-22.92412|-43.28238|1694718005000|        30| 2111|1694718101000|   1694718109000|\n",
      "|C30092|-22.87622|-43.24181|1694718093000|        27|  343|1694718101000|   1694718109000|\n",
      "|C47919|-22.91208|-43.20929|1694718089000|         0| 9909|1694718101000|   1694718109000|\n",
      "|C47970|-22.93863|-43.34613|1694717966000|         0| 2114|1694718101000|   1694718109000|\n",
      "+------+---------+---------+-------------+----------+-----+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dest_folder = \"/user/root/bus/test.parquet\"\n",
    "df = spark.read.parquet(dest_folder)\n",
    "\n",
    "# Exiba o DataFrame lido\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "2a8d59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_batch, dir_stream = \"/user/root/onibus/batch/*/*\", \"/user/root/onibus/stream/*/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "8706e713",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc = spark.read.schema(schema).json(dir_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "4b270c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------+---------+--------+----------+-----+-------------+----------------+\n",
      "|ordem|latitude|longitude|datahora|velocidade|linha|datahoraenvio|datahoraservidor|\n",
      "+-----+--------+---------+--------+----------+-----+-------------+----------------+\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "| null|    null|     null|    null|      null| null|         null|            null|\n",
      "+-----+--------+---------+--------+----------+-----+-------------+----------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6590fa40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2875, 8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad013e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc2 = spark.read.csv(dir_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80f564bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+-------------+---+------+-------------+-------------+\n",
      "|   _c0|      _c1|      _c2|          _c3|_c4|   _c5|          _c6|          _c7|\n",
      "+------+---------+---------+-------------+---+------+-------------+-------------+\n",
      "|B28555| -22.7981|-43.19024|1694717767000|  0|   329|1694717772000|1694717774000|\n",
      "|B28738|-22.82059|-43.17902|1694717357000| 16|  2343|1694717772000|1694717774000|\n",
      "|B28733|  -22.897|-43.18719|1694717766000| 11|  2343|1694717772000|1694717774000|\n",
      "|B28702|-22.90718| -43.1739|1694717760000|  9|  2344|1694717772000|1694717774000|\n",
      "|B28738|-22.82079|-43.17818|1694717373000| 23|  2343|1694717772000|1694717774000|\n",
      "|B28702|-22.90713|-43.17395|1694717766000|  7|  2344|1694717772000|1694717774000|\n",
      "|B28602|-22.89278|-43.19396|1694717764000| 72|   492|1694717772000|1694717774000|\n",
      "|B28738| -22.8215|-43.17757|1694717403000| 18|  2343|1694717772000|1694717774000|\n",
      "|B28738|-22.82335|-43.17601|1694717433000| 28|  2343|1694717772000|1694717774000|\n",
      "|B28522|-22.79552| -43.1949|1694717767000| 47|   492|1694717772000|1694717774000|\n",
      "|B28738|-22.82423|-43.17364|1694717463000| 31|  2343|1694717772000|1694717774000|\n",
      "|B28582|-22.79052|-43.18084|1694717767000|  0|   329|1694717772000|1694717774000|\n",
      "|B28511|-22.81276|-43.20765|1694717768000| 14|   324|1694717772000|1694717774000|\n",
      "|B28515|-22.81641|-43.18774|1694717767000| 15|   325|1694717772000|1694717774000|\n",
      "|B28738|-22.82544|-43.17227|1694717488000| 21|  2343|1694717772000|1694717774000|\n",
      "|B28623|-22.79472|-43.18006|1694717771000| 27|LECD61|1694717772000|1694717774000|\n",
      "|B28738|-22.82523|-43.17148|1694717518000| 23|  2343|1694717772000|1694717774000|\n",
      "|B28554|-22.79521|-43.18057|1694717769000|  6|   321|1694717772000|1694717774000|\n",
      "|B28702|-22.90718|-43.17399|1694717769000|  7|  2344|1694717772000|1694717774000|\n",
      "|B28738|-22.82501|-43.17046|1694717548000| 16|  2343|1694717772000|1694717774000|\n",
      "+------+---------+---------+-------------+---+------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfc2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b8ef7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94758, 8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(dfc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c3d7dc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfc2.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
