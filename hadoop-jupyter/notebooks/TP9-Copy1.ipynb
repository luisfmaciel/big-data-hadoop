{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "d961fdce",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import requests\n",
    "import datetime as dt\n",
    "from multiprocessing import Process\n",
    "\n",
    "import os\n",
    "import findspark\n",
    "import pandas as pd\n",
    "findspark.init()\n",
    "from pathlib import Path\n",
    "from pyspark import SparkContext, SparkConf, SQLContext\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, FloatType, LongType, TimestampType\n",
    "from pyspark.sql.functions import col, regexp_replace, row_number, when, sum, count, round, date_format, lit, first\n",
    "from pyspark.sql import SparkSession, Window\n",
    "findspark.init('/opt/spark')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "651bbd16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following additional packages will be installed:\n",
      "  libcurl4 libnghttp2-14 librtmp1\n",
      "The following NEW packages will be installed:\n",
      "  curl libcurl4 libnghttp2-14 librtmp1\n",
      "0 upgraded, 4 newly installed, 0 to remove and 0 not upgraded.\n",
      "Need to get 511 kB of archives.\n",
      "After this operation, 1399 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libnghttp2-14 amd64 1.30.0-1ubuntu1 [77.8 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 librtmp1 amd64 2.4+20151223.gitfa8646d.1-1 [54.2 kB]\n",
      "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 libcurl4 amd64 7.58.0-2ubuntu3.24 [221 kB]\n",
      "Get:4 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 curl amd64 7.58.0-2ubuntu3.24 [159 kB]\n",
      "Fetched 511 kB in 6s (86.1 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package libnghttp2-14:amd64.\n",
      "(Reading database ... 35561 files and directories currently installed.)\n",
      "Preparing to unpack .../libnghttp2-14_1.30.0-1ubuntu1_amd64.deb ...\n",
      "Unpacking libnghttp2-14:amd64 (1.30.0-1ubuntu1) ...\n",
      "Selecting previously unselected package librtmp1:amd64.\n",
      "Preparing to unpack .../librtmp1_2.4+20151223.gitfa8646d.1-1_amd64.deb ...\n",
      "Unpacking librtmp1:amd64 (2.4+20151223.gitfa8646d.1-1) ...\n",
      "Selecting previously unselected package libcurl4:amd64.\n",
      "Preparing to unpack .../libcurl4_7.58.0-2ubuntu3.24_amd64.deb ...\n",
      "Unpacking libcurl4:amd64 (7.58.0-2ubuntu3.24) ...\n",
      "Selecting previously unselected package curl.\n",
      "Preparing to unpack .../curl_7.58.0-2ubuntu3.24_amd64.deb ...\n",
      "Unpacking curl (7.58.0-2ubuntu3.24) ...\n",
      "Setting up libnghttp2-14:amd64 (1.30.0-1ubuntu1) ...\n",
      "Setting up librtmp1:amd64 (2.4+20151223.gitfa8646d.1-1) ...\n",
      "Setting up libcurl4:amd64 (7.58.0-2ubuntu3.24) ...\n",
      "Setting up curl (7.58.0-2ubuntu3.24) ...\n",
      "Processing triggers for libc-bin (2.27-3ubuntu1.6) ...\n"
     ]
    }
   ],
   "source": [
    "! apt-get install -y curl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7eb93138",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1056k  100 1056k    0     0   327k      0  0:00:03  0:00:03 --:--:--  327k\n"
     ]
    }
   ],
   "source": [
    "! curl -O https://jdbc.postgresql.org/download/postgresql-42.6.0.jar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5215ec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\n",
    "    \"tp9_teste1\"\n",
    ").config(\n",
    "    \"spark.jars\",\n",
    "    f\"{str(Path.cwd())}/postgresql-42.6.0.jar\"\n",
    ").config(\n",
    "    \"spark.driver.extraClassPath\",\n",
    "    f\"{str(Path.cwd())}/postgresql-42.6.0.jar\"\n",
    ").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "99205154",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir -p /user/root/bus/batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "db068786",
   "metadata": {},
   "outputs": [],
   "source": [
    "! hadoop fs -mkdir -p /user/root/bus/stream"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "a896754f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\r\n",
      "drwxr-xr-x   - root supergroup          0 2023-09-17 18:29 /user/root/bus/batch/calc_traffic/20230917_152900\r\n",
      "drwxr-xr-x   - root supergroup          0 2023-09-17 18:29 /user/root/bus/batch/calc_traffic/20230917_152920\r\n",
      "drwxr-xr-x   - root supergroup          0 2023-09-17 18:29 /user/root/bus/batch/calc_traffic/20230917_152940\r\n",
      "drwxr-xr-x   - root supergroup          0 2023-09-17 18:30 /user/root/bus/batch/calc_traffic/20230917_153020\r\n",
      "drwxr-xr-x   - root supergroup          0 2023-09-17 18:30 /user/root/bus/batch/calc_traffic/20230917_153040\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -ls /user/root/bus/batch/calc_traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "fa4c3e33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted /user/root/bus/stream/20230917_162108\r\n",
      "Deleted /user/root/bus/stream/20230917_162135\r\n",
      "Deleted /user/root/bus/stream/20230917_162202\r\n",
      "Deleted /user/root/bus/stream/20230917_162229\r\n",
      "Deleted /user/root/bus/stream/20230917_162256\r\n"
     ]
    }
   ],
   "source": [
    "! hadoop fs -rm -skipTrash -r /user/root/bus/stream/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "ee402bbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shape(df):\n",
    "    return (df.count(), len(df.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "5f6107a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_URL = \"https://dados.mobilidade.rio/gps/sppo?\" \\\n",
    "           \"dataInicial={dt_inicial}+{hora_inicial}&dataFinal={dt_final}+{hora_final}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "f6d9ed18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_bus_api(interval=20):\n",
    "    offset = dt.timedelta(hours=3)\n",
    "    dtf = dt.datetime.now() - offset\n",
    "    dti = dtf - dt.timedelta(seconds=interval)\n",
    "    dt_inicial = dti.strftime(\"%Y-%m-%d\")\n",
    "    dt_final = dtf.strftime(\"%Y-%m-%d\")\n",
    "    hora_inicial = dti.strftime(\"%H:%M:%S\")\n",
    "    hora_final = dtf.strftime(\"%H:%M:%S\")\n",
    "    ret = requests.get(\n",
    "        BASE_URL.format(\n",
    "            dt_inicial=dt_inicial, dt_final=dt_final, hora_inicial=hora_inicial, hora_final=hora_final\n",
    "        )\n",
    "    )\n",
    "    rdd = spark.sparkContext.parallelize([ret.text])\n",
    "    df = spark.read.json(rdd)\n",
    "    df = df.withColumn(\n",
    "        \"hora\", lit(dtf.strftime('%H:%M:%S'))\n",
    "    )\n",
    "    df.write.json(f\"/user/root/bus/stream/{dtf.strftime('%Y%m%d_%H%M%S')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "e007be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [\n",
    "    \"traffic_vs_garage\"\n",
    "#     \"percent_above_60km\",\n",
    "#     \"qty_bus_running_by_company\",\n",
    "#     \"qty_bus_running_by_line\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "ef88cf79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table(df):\n",
    "    for table in tables:\n",
    "        jdbc_url = \"jdbc:postgresql://pg-data:5433/postgres\"\n",
    "        database_table = f\"\\\"{table}\\\"\" \n",
    "        properties = {\n",
    "            \"user\": \"postgres\",\n",
    "            \"password\": \"postgres\",\n",
    "            \"driver\": \"org.postgresql.Driver\"\n",
    "        }\n",
    "\n",
    "        df.write.jdbc(url=jdbc_url, table=database_table, mode=\"append\", properties=properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "86d68a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_traffic(df, dtt):\n",
    "    df = df.withColumn(\n",
    "        \"is_garage\", when(col(\"linha\") == \"GARAGEM\", 1).otherwise(0)\n",
    "    )\n",
    "\n",
    "    df = df.groupBy(\"hora\").agg(\n",
    "        round((sum(\"is_garage\") / count(\"*\") * 100), 2).alias(\"percent_garage\"), \n",
    "        round(((count(\"*\") - sum(\"is_garage\")) / count(\"*\") * 100), 2).alias(\"percent_traffic\")\n",
    "    )\n",
    "\n",
    "#     df = df.select(\n",
    "#         round((sum(\"is_garage\") / count(\"*\") * 100), 2).alias(\"percent_garage\"), \n",
    "#         round(((count(\"*\") - sum(\"is_garage\")) / count(\"*\") * 100), 2).alias(\"percent_traffic\")\n",
    "#     )\n",
    "        \n",
    "    df.show()\n",
    "    \n",
    "    create_table(df)\n",
    "    \n",
    "    df.write.parquet(f\"/user/root/bus/batch/calc_traffic/{dtt}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "c50e5bd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_stream(bdf, ctrl):\n",
    "    offset = dt.timedelta(hours=3)\n",
    "    dtt = dt.datetime.now() - offset\n",
    "\n",
    "    dtt = dtt.strftime('%Y%m%d_%H%M%S')\n",
    "    calc_traffic(bdf, dtt)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "ac25e4c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"linha\", StringType(), True),\n",
    "    StructField(\"hora\", StringType(), True),\n",
    "    StructField(\"percent_garage\", StringType(), True),\n",
    "    StructField(\"percent_traffic\", StringType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "56ea6f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loop(max_iter=5):\n",
    "    idx = 1\n",
    "    while True:\n",
    "        print(f\"{idx} - fetching API\")\n",
    "        call_bus_api()\n",
    "        if idx == max_iter:\n",
    "            break\n",
    "        time.sleep(25)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "2bfd8e9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = spark.readStream.schema(schema).json(\"/user/root/bus/stream/*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "b810c5b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_handle = sdf.writeStream                         \\\n",
    "                   .outputMode(\"append\")                \\\n",
    "                   .foreachBatch(handle_stream)         \\\n",
    "                   .trigger(processingTime=\"20 seconds\") \\\n",
    "                   .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "a56420c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 200,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_handle.isActive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "3ee52f11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Waiting for next trigger',\n",
       " 'isDataAvailable': False,\n",
       " 'isTriggerActive': False}"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_handle.status"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "711feca6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'id': '5b3fe05d-ac86-4adb-884d-80e62b5b0133',\n",
       " 'runId': '1cd0f46c-d35f-4938-9835-5be339bbba01',\n",
       " 'name': None,\n",
       " 'timestamp': '2023-09-17T19:25:32.967Z',\n",
       " 'batchId': 0,\n",
       " 'numInputRows': 0,\n",
       " 'processedRowsPerSecond': 0.0,\n",
       " 'durationMs': {'getOffset': 1, 'triggerExecution': 2},\n",
       " 'stateOperators': [],\n",
       " 'sources': [{'description': 'FileStreamSource[hdfs://node-master:9000/user/root/bus/stream/*]',\n",
       "   'startOffset': None,\n",
       "   'endOffset': None,\n",
       "   'numInputRows': 0,\n",
       "   'processedRowsPerSecond': 0.0}],\n",
       " 'sink': {'description': 'ForeachBatchSink'}}"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stream_handle.lastProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "dc737885",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 - fetching API\n",
      "+--------+--------------+---------------+\n",
      "|    hora|percent_garage|percent_traffic|\n",
      "+--------+--------------+---------------+\n",
      "|16:25:34|         15.25|          84.75|\n",
      "+--------+--------------+---------------+\n",
      "\n",
      "2 - fetching API\n",
      "3 - fetching API\n",
      "4 - fetching API\n",
      "5 - fetching API\n"
     ]
    }
   ],
   "source": [
    "p = Process(target=loop)\n",
    "p.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c694ad40",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd61dad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4a5d3a7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "1f6511e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "p.terminate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "8ab0440b",
   "metadata": {},
   "outputs": [],
   "source": [
    "stream_handle.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "34e5dd04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------------+\n",
      "|percent_garage|percent_traffic|\n",
      "+--------------+---------------+\n",
      "|         11.47|          88.53|\n",
      "+--------------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "diretorio = \"/user/root/bus/batch/calc_traffic\"\n",
    "\n",
    "lista_arquivos = os.listdir(diretorio)\n",
    "\n",
    "for \n",
    "dest_folder = \"/user/root/bus/batch/calc_traffic/20230917_152900\"\n",
    "df = spark.read.parquet(dest_folder)\n",
    "\n",
    "# Exiba o DataFrame lido\n",
    "df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2a8d59e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_batch, dir_stream = \"/user/root/onibus/batch/*/*\", \"/user/root/onibus/stream/*/*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "8706e713",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "'Path does not exist: hdfs://node-master:9000/user/root/onibus/stream/*/*;'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mPy4JJavaError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mpy4j\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprotocol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPy4JJavaError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[0;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[1;32m    327\u001b[0m                     \u001b[0;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[1;32m    329\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mPy4JJavaError\u001b[0m: An error occurred while calling o1394.json.\n: org.apache.spark.sql.AnalysisException: Path does not exist: hdfs://node-master:9000/user/root/onibus/stream/*/*;\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:570)\n\tat org.apache.spark.sql.execution.datasources.DataSource$$anonfun$org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary$1.apply(DataSource.scala:559)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.TraversableLike$$anonfun$flatMap$1.apply(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat scala.collection.TraversableLike$class.flatMap(TraversableLike.scala:241)\n\tat scala.collection.immutable.List.flatMap(List.scala:355)\n\tat org.apache.spark.sql.execution.datasources.DataSource.org$apache$spark$sql$execution$datasources$DataSource$$checkAndGlobPathIfNecessary(DataSource.scala:559)\n\tat org.apache.spark.sql.execution.datasources.DataSource.resolveRelation(DataSource.scala:373)\n\tat org.apache.spark.sql.DataFrameReader.loadV1Source(DataFrameReader.scala:242)\n\tat org.apache.spark.sql.DataFrameReader.load(DataFrameReader.scala:230)\n\tat org.apache.spark.sql.DataFrameReader.json(DataFrameReader.scala:411)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\n\tat py4j.Gateway.invoke(Gateway.java:282)\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\n\tat py4j.GatewayConnection.run(GatewayConnection.java:238)\n\tat java.lang.Thread.run(Thread.java:750)\n",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mAnalysisException\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-167-8417d66a0f07>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mschema\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdir_stream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/readwriter.py\u001b[0m in \u001b[0;36mjson\u001b[0;34m(self, path, schema, primitivesAsString, prefersDecimal, allowComments, allowUnquotedFieldNames, allowSingleQuotes, allowNumericLeadingZero, allowBackslashEscapingAnyCharacter, mode, columnNameOfCorruptRecord, dateFormat, timestampFormat, multiLine, allowUnquotedControlChars, lineSep, samplingRatio, dropFieldIfAllNull, encoding)\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0mpath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    273\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 274\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPythonUtils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoSeq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    275\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRDD\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m             \u001b[0;32mdef\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/lib/py4j-0.10.7-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m         return_value = get_return_value(\n\u001b[0;32m-> 1257\u001b[0;31m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[1;32m   1258\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/spark/python/pyspark/sql/utils.py\u001b[0m in \u001b[0;36mdeco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m     67\u001b[0m                                              e.java_exception.getStackTrace()))\n\u001b[1;32m     68\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.AnalysisException: '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'org.apache.spark.sql.catalyst.analysis'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mAnalysisException\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m': '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstackTrace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: 'Path does not exist: hdfs://node-master:9000/user/root/onibus/stream/*/*;'"
     ]
    }
   ],
   "source": [
    "dfc = spark.read.schema(schema).json(dir_stream)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "4b270c5a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-168-4164da21ceeb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdfc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'dfc' is not defined"
     ]
    }
   ],
   "source": [
    "dfc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "6590fa40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2875, 8)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(dfc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ad013e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfc2 = spark.read.csv(dir_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "80f564bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------+-------------+---+------+-------------+-------------+\n",
      "|   _c0|      _c1|      _c2|          _c3|_c4|   _c5|          _c6|          _c7|\n",
      "+------+---------+---------+-------------+---+------+-------------+-------------+\n",
      "|B28555| -22.7981|-43.19024|1694717767000|  0|   329|1694717772000|1694717774000|\n",
      "|B28738|-22.82059|-43.17902|1694717357000| 16|  2343|1694717772000|1694717774000|\n",
      "|B28733|  -22.897|-43.18719|1694717766000| 11|  2343|1694717772000|1694717774000|\n",
      "|B28702|-22.90718| -43.1739|1694717760000|  9|  2344|1694717772000|1694717774000|\n",
      "|B28738|-22.82079|-43.17818|1694717373000| 23|  2343|1694717772000|1694717774000|\n",
      "|B28702|-22.90713|-43.17395|1694717766000|  7|  2344|1694717772000|1694717774000|\n",
      "|B28602|-22.89278|-43.19396|1694717764000| 72|   492|1694717772000|1694717774000|\n",
      "|B28738| -22.8215|-43.17757|1694717403000| 18|  2343|1694717772000|1694717774000|\n",
      "|B28738|-22.82335|-43.17601|1694717433000| 28|  2343|1694717772000|1694717774000|\n",
      "|B28522|-22.79552| -43.1949|1694717767000| 47|   492|1694717772000|1694717774000|\n",
      "|B28738|-22.82423|-43.17364|1694717463000| 31|  2343|1694717772000|1694717774000|\n",
      "|B28582|-22.79052|-43.18084|1694717767000|  0|   329|1694717772000|1694717774000|\n",
      "|B28511|-22.81276|-43.20765|1694717768000| 14|   324|1694717772000|1694717774000|\n",
      "|B28515|-22.81641|-43.18774|1694717767000| 15|   325|1694717772000|1694717774000|\n",
      "|B28738|-22.82544|-43.17227|1694717488000| 21|  2343|1694717772000|1694717774000|\n",
      "|B28623|-22.79472|-43.18006|1694717771000| 27|LECD61|1694717772000|1694717774000|\n",
      "|B28738|-22.82523|-43.17148|1694717518000| 23|  2343|1694717772000|1694717774000|\n",
      "|B28554|-22.79521|-43.18057|1694717769000|  6|   321|1694717772000|1694717774000|\n",
      "|B28702|-22.90718|-43.17399|1694717769000|  7|  2344|1694717772000|1694717774000|\n",
      "|B28738|-22.82501|-43.17046|1694717548000| 16|  2343|1694717772000|1694717774000|\n",
      "+------+---------+---------+-------------+---+------+-------------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfc2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2b8ef7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94758, 8)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "shape(dfc2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0c3d7dc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: string (nullable = true)\n",
      " |-- _c1: string (nullable = true)\n",
      " |-- _c2: string (nullable = true)\n",
      " |-- _c3: string (nullable = true)\n",
      " |-- _c4: string (nullable = true)\n",
      " |-- _c5: string (nullable = true)\n",
      " |-- _c6: string (nullable = true)\n",
      " |-- _c7: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dfc2.printSchema()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
